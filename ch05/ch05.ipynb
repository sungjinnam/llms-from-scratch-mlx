{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4738cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c0622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bf8ad",
   "metadata": {},
   "source": [
    "# Evaluating text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ff7693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.09999999999999998)\n",
       "  (trf_blocks): Sequential(\n",
       "    (layers.0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(input_dims=768, output_dims=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "\n",
    "mx.random.seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe17449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = mx.array(encoded)[None, :]\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze().tolist()\n",
    "    return tokenizer.decode(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4a1830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you DMVSK DMVieversseparSKseparseparseparsepar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7a4c6",
   "metadata": {},
   "source": [
    "## Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32900a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mx.array([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                   [40, 1107, 588]])      #  \"I really like\"]\n",
    "targets = mx.array([[3626, 6100, 345],    # [\" effort moves you\"\n",
    "                    [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d29abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1.08953e-05, 1.54743e-05, 5.29234e-05, ..., 1.1125e-05, 1.47884e-05, 1.07763e-05],\n",
       "         [1.16216e-05, 2.01177e-05, 6.33257e-05, ..., 1.26e-05, 8.72894e-06, 1.6723e-05],\n",
       "         [1.16933e-05, 2.81832e-05, 5.13414e-05, ..., 1.21165e-05, 1.2233e-05, 1.76848e-05]],\n",
       "        [[1.78601e-05, 1.22162e-05, 1.27557e-05, ..., 2.22658e-05, 2.96495e-05, 3.25666e-06],\n",
       "         [1.47091e-05, 1.38527e-05, 1.48364e-05, ..., 2.22301e-05, 2.02625e-05, 4.84638e-06],\n",
       "         [1.36925e-05, 1.54772e-05, 1.60843e-05, ..., 2.20074e-05, 1.94253e-05, 6.39406e-06]]], dtype=float32),\n",
       " (2, 3, 50257))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(mx.stop_gradient(inputs))\n",
    "probas = nn.softmax(logits, axis=-1)\n",
    "probas, probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1a6058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[33548],\n",
       "        [38162],\n",
       "        [38162]],\n",
       "       [[16272],\n",
       "        [16272],\n",
       "        [16272]]], dtype=uint32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = mx.argmax(probas, axis=-1, keepdims=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac0c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: 501YRYR\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e525799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: array([1.9632e-05, 1.4344e-05, 1.21008e-05], dtype=float32)\n",
      "Text 2: array([1.32206e-05, 6.42074e-06, 1.168e-05], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b911e1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.8384, -11.1522, -11.3222, -11.2337, -11.956, -11.3576], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to maximize the log probability of the targets\n",
    "log_probas = mx.log(mx.concat([target_probas_1, target_probas_2], axis=0))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb698e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-11.31, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probas = mx.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d92363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(11.31, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in training, we minimize the negative log likelihood\n",
    "neg_avg_log_probas = -avg_log_probas\n",
    "neg_avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9198c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 50257), (6,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.flatten(0, 1).shape, targets.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d63b4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(11.31, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.losses.cross_entropy(logits.flatten(0, 1), targets.flatten(),\n",
    "                               reduction=\"mean\")\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02a1d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(81635.5, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity is exp of the cross-entropy loss\n",
    "perplexity = mx.exp(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390277b6",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5a62088",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ch02/the-verdict.txt\", \"r\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adedbea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no ',\n",
       " 'it for me! The Strouds stand alone, and happen once--but there\\'s no exterminating our kind of art.\"')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:99], text_data[-99:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b27811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20479, 5145)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_characters, total_tokens = len(text_data), len(tokenizer.encode(text_data))\n",
    "total_characters, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1aa9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "\n",
      "Validation loader:\n",
      "(2, 256) (2, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4608, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from previous_chapters import GPTDatasetV1\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "mx.random.seed(123)\n",
    "train_loader = GPTDatasetV1(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = GPTDatasetV1(\n",
    "    val_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the trainig loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "    \n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "train_tokens = 0\n",
    "for ex in iter(train_loader):\n",
    "    print(ex['input_ids'].shape, ex['target_ids'].shape)\n",
    "    train_tokens += ex['input_ids'].size\n",
    "\n",
    "val_tokens = 0\n",
    "print(\"\\nValidation loader:\")\n",
    "for ex in iter(val_loader):\n",
    "    print(ex['input_ids'].shape, ex['target_ids'].shape)\n",
    "    val_tokens += ex['input_ids'].size\n",
    "\n",
    "train_tokens, val_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f3cad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce_loss(logits, target):\n",
    "    loss = nn.losses.cross_entropy(logits.flatten(0, 1), target.flatten(), \n",
    "                                   reduction=\"mean\")\n",
    "    return loss\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, no_grad, device=None):\n",
    "    input_batch, target_batch = mx.array(input_batch), mx.array(target_batch)\n",
    "\n",
    "    if no_grad:\n",
    "        input_batch = mx.stop_gradient(input_batch)\n",
    "    logits = model(input_batch)\n",
    "    batch_step_fn = nn.value_and_grad(model, compute_ce_loss)\n",
    "    loss, grad = batch_step_fn(logits, target_batch)\n",
    "    return loss, grad\n",
    "\n",
    "def calc_loss_loader(data_loader, model, no_grad, device=None, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, ex in enumerate(iter(data_loader)):\n",
    "        if i < num_batches:\n",
    "            loss, _ = calc_loss_batch(ex['input_ids'], ex['target_ids'], model, no_grad, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70255990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.026213222079807, 11.002269744873047)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(123)\n",
    "train_loss = calc_loss_loader(train_loader, model, True)\n",
    "val_loss = calc_loss_loader(val_loader, model, True)\n",
    "train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad4453",
   "metadata": {},
   "source": [
    "# Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c2c314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Device(gpu, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: set device for model and data functions\n",
    "mx.default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd156eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, \n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # mx.eval(model.parameters())\n",
    "        model.train()\n",
    "        for batch in iter(train_loader):\n",
    "            input_batch, target_batch = batch['input_ids'], batch['target_ids']\n",
    "            loss, grads = calc_loss_batch(input_batch, target_batch, model, no_grad=False)\n",
    "            optimizer.update(model, grads)\n",
    "            # Force a graph evaluation\n",
    "            # mx.eval(model.parameters(), optimizer.state)\n",
    "            # mx.eval(model.state)\n",
    "            mx.eval(loss, [model.state, optimizer.state])\n",
    "            tokens_seen += input_batch.size\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1}, step {global_step:06d}: \"\n",
    "                      f\"train loss {train_loss:.3f}, val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {loss.item():.3f}, Tokens seen: {tokens_seen}\")\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    train_loss = calc_loss_loader(train_loader, model, True, device, eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, True, device, eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=mx.stop_gradient(encoded),\n",
    "        max_new_tokens=50,\n",
    "        context_size=context_size,\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5ff2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 000005: train loss 11.036, val loss 11.002\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 2, step 000010: train loss 11.034, val loss 11.002\n",
      "Epoch 2, step 000015: train loss 11.031, val loss 11.002\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 3, step 000020: train loss 11.026, val loss 11.002\n",
      "Epoch 3, step 000025: train loss 11.021, val loss 11.002\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 4, step 000030: train loss 11.021, val loss 11.001\n",
      "Epoch 4, step 000035: train loss 11.028, val loss 11.001\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 5, step 000040: train loss 11.004, val loss 11.001\n",
      "Epoch 5, step 000045: train loss 11.032, val loss 11.001\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 6, step 000050: train loss 11.016, val loss 11.001\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 7, step 000055: train loss 11.011, val loss 11.001\n",
      "Epoch 7, step 000060: train loss 11.047, val loss 11.001\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 8, step 000065: train loss 11.009, val loss 11.000\n",
      "Epoch 8, step 000070: train loss 11.039, val loss 11.000\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 9, step 000075: train loss 11.026, val loss 11.000\n",
      "Epoch 9, step 000080: train loss 11.047, val loss 11.000\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n",
      "Epoch 10, step 000085: train loss 11.034, val loss 11.000\n",
      "Epoch 10, step 000090: train loss 11.030, val loss 11.000\n",
      "Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSKSKSKSKSK hrsSK hrsSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSKSK\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "optimizer = optim.AdamW(learning_rate=4e-4, weight_decay=1e-1)\n",
    "\n",
    "# TODO: training loss is decreasing too slow, needs investigation.\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, None, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74195dd0",
   "metadata": {},
   "source": [
    "# Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b3ed255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text:\n",
      " Every effort moves you DMVSK DMVieversseparSKseparseparseparseparseparseparseparseparSKSKSK hrsSK hrsSKSKSK hrsSK\n"
     ]
    }
   ],
   "source": [
    "# generating the same text output\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "print(\"output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "010ffa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forward'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temperature scaling and sample from the distribution, instead of using argmax\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# suppose input is \"every effort moves you\", and the LLM returns the following logits for the next token:\n",
    "next_token_logits = mx.array([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "probas = nn.softmax(next_token_logits, axis=-1)\n",
    "next_token_id = mx.argmax(probas).item()\n",
    "inverse_vocab[next_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6965841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.0 x closer\n",
      "87.0 x every\n",
      "98.0 x effort\n",
      "177.0 x forward\n",
      "100.0 x inches\n",
      "93.0 x moves\n",
      "98.0 x pizza\n",
      "137.0 x toward\n",
      "103.0 x you\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(123)\n",
    "sample = [mx.random.categorical(probas, num_samples=1).item() for _ in range(1000)]\n",
    "sampled_ids = mx.zeros(len(vocab)).at[sample].add(1)\n",
    "for i, freq in enumerate(sampled_ids):\n",
    "    print(f\"{freq} x {inverse_vocab[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f96e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature sampling\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return nn.softmax(scaled_logits, axis=0)\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae8579e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, ..., 6, 7, 8], dtype=int32),\n",
       " array([0.0609071, 0.00163125, 0.000100194, ..., 0.000101201, 0.357576, 0.00401224], dtype=float32))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, scaled_probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5431cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = mx.arange(len(vocab))\n",
    "# bar_width = 0.15\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots(figsize=(5, 3))\n",
    "# for i, T in enumerate(temperatures):\n",
    "#     rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature={T}')\n",
    "\n",
    "# ax.set_ylabel('Probability')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "# ax.legend()\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f50edcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.51, 6.28, 6.75], dtype=float32), array([3, 7, 0], dtype=uint32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top-k sampling\n",
    "top_k = 3\n",
    "top_logits = mx.topk(next_token_logits, top_k)\n",
    "top_pos = mx.argsort(next_token_logits)[::-1][:top_k]\n",
    "top_logits, top_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "743dd5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.51, -inf, -inf, ..., -inf, 6.28, -inf], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_logits = mx.where(\n",
    "    next_token_logits < top_logits.min(),\n",
    "    -mx.inf,\n",
    "    next_token_logits,\n",
    ")\n",
    "new_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11f409a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated generation function with temperature and top-k sampling\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits = model(mx.stop_gradient(idx_cond))\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # top_k filtering\n",
    "        if top_k is not None:\n",
    "            top_logits = mx.topk(logits, top_k)\n",
    "            min_val = top_logits.min()\n",
    "            logits = mx.where(logits < min_val, -mx.inf, logits)\n",
    "        \n",
    "        # temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = mx.softmax(logits, axis=-1)\n",
    "            idx_next = mx.random.categorical(probs, num_samples=1)\n",
    "        # otherwise, use the highest logit value\n",
    "        else:\n",
    "            idx_next = mx.argmax(logits, axis=-1, keepdims=True)\n",
    "\n",
    "        if idx_next.item() == eos_id:\n",
    "            break\n",
    "        idx = mx.concat([idx, idx_next], axis=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2833e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you783 hallmark Cherpass indicator identifies sector lair riteictionaryAlong Hampshirelaughs sons mocked\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ed49b",
   "metadata": {},
   "source": [
    "# loading and saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a safetensor file\n",
    "model.save_weights(\"__model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff622c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.09999999999999998)\n",
       "  (trf_blocks): Sequential(\n",
       "    (layers.0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(input_dims=768, output_dims=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from a file\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_weights(\"__model.safetensors\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f18f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model and optimizer state\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "checkpoint = {}\n",
    "tree_flatten(model.state, destination=checkpoint, prefix=\"_model\")\n",
    "tree_flatten(optimizer.state, destination=checkpoint, prefix=\"_optimizer\")\n",
    "\n",
    "mx.save_safetensors(\"__model_and_optimizer.safetensors\", checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d84c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from a checkpoint file\n",
    "from mlx.utils import tree_unflatten\n",
    "loaded_checkpoint = mx.load(\"__model_and_optimizer.safetensors\")\n",
    "\n",
    "model_state = {k.replace(\"model.\", \"\"): v for k, v in loaded_checkpoint.items() if k.startswith(\"model\")}\n",
    "opt_state = {k.replace(\"optimizer.\", \"\"): v for k, v in loaded_checkpoint.items() if k.startswith(\"optimizer\")}\n",
    "\n",
    "model.update(tree_unflatten(model_state))\n",
    "optimizer.state = opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af7039",
   "metadata": {},
   "source": [
    "# Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "83d5db82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to gpt2-small-124M.pth\n"
     ]
    }
   ],
   "source": [
    "import os, urllib\n",
    "file_name = \"gpt2-small-124M.pth\"\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fb12ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pytorch_state_dict = torch.load(\"gpt2-small-124M.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "31178ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmx\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpytorch_state_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py:206\u001b[39m, in \u001b[36mModule.load_weights\u001b[39m\u001b[34m(self, file_or_weights, strict)\u001b[39m\n\u001b[32m    200\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but received \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv_new.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m             )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weights) != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_unflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py:356\u001b[39m, in \u001b[36mModule.update\u001b[39m\u001b[34m(self, parameters, strict)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m strict:\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived invalid type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameters).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py:338\u001b[39m, in \u001b[36mModule.update.<locals>.apply\u001b[39m\u001b[34m(dst, parameters)\u001b[39m\n\u001b[32m    336\u001b[39m         dst[k] = new_value\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m         \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m strict:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule does not have parameter named \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py:343\u001b[39m, in \u001b[36mModule.update.<locals>.apply\u001b[39m\u001b[34m(dst, parameters)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parameters, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(parameters)):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m         current_value = \u001b[43mdst\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    344\u001b[39m         new_value = parameters[i]\n\u001b[32m    345\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_value, mx.array):\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "model.load_weights(\n",
    "    {k: mx.array(v.numpy()) for k, v in pytorch_state_dict.items()},\n",
    "    strict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4841ac5b614a248d4e49bb53b4e7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ab75c295be47739ba0236d7013ffe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6528cb2ac0c45a6896ebfb518d2eaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020dc21688d14241be89f789f430bb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7c755ed2f54f18b2d9d0a23bfa25f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5702c4a92dfa4b08aa60400322d2d28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ba7a486a394c0686e26141d8a05fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3584a429934c41afe3ff094556836b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc385d6487e4b70a3a197c103703334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeead2a458f4a19a28d6e9bf8402fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0c6c74e1a45a4bcaa54c144647fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578985adff19407fb98400274f0cf3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c19040bc8b4cae98151c64eda637d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b395c7938bc842789c868863cdb58deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative: huggingface-mlx support=\n",
    "from mlx_lm import load, generate\n",
    "model_hf, tokenizer_hf = load(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71fe5b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "forward.\n",
      "\n",
      "The first step is to understand the importance of your work.\n",
      "\n",
      "The second step is to understand the importance of your work.\n",
      "\n",
      "The third step is to understand the importance of your work.\n",
      "\n",
      "The fourth step is to understand the importance of your work.\n",
      "\n",
      "The fifth step is to understand the importance of your work.\n",
      "\n",
      "The sixth step is to understand the importance of your work.\n",
      "\n",
      "The seventh step is to understand the importance of your work.\n",
      "\n",
      "The eighth step is to understand the importance of your work.\n",
      "\n",
      "The ninth step is to understand the importance of your work.\n",
      "\n",
      "The tenth step is to understand the importance of your work.\n",
      "\n",
      "The eleventh step is to understand the importance of your work.\n",
      "\n",
      "The twelfth step is to understand the importance of your work.\n",
      "\n",
      "The thirteenth step is to understand the importance of your work.\n",
      "\n",
      "The thirteenth step is to understand the importance of your work.\n",
      "\n",
      "The thirteenth step is to understand the importance of your work.\n",
      "\n",
      "The thirteenth step is to understand the importance of your work.\n",
      "\n",
      "The thirteenth step is to understand the importance of your work.\n",
      "\n",
      "The th\n",
      "==========\n",
      "Prompt: 4 tokens, 7.111 tokens-per-sec\n",
      "Generation: 256 tokens, 154.787 tokens-per-sec\n",
      "Peak memory: 11.879 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'forward.\\n\\nThe first step is to understand the importance of your work.\\n\\nThe second step is to understand the importance of your work.\\n\\nThe third step is to understand the importance of your work.\\n\\nThe fourth step is to understand the importance of your work.\\n\\nThe fifth step is to understand the importance of your work.\\n\\nThe sixth step is to understand the importance of your work.\\n\\nThe seventh step is to understand the importance of your work.\\n\\nThe eighth step is to understand the importance of your work.\\n\\nThe ninth step is to understand the importance of your work.\\n\\nThe tenth step is to understand the importance of your work.\\n\\nThe eleventh step is to understand the importance of your work.\\n\\nThe twelfth step is to understand the importance of your work.\\n\\nThe thirteenth step is to understand the importance of your work.\\n\\nThe thirteenth step is to understand the importance of your work.\\n\\nThe thirteenth step is to understand the importance of your work.\\n\\nThe thirteenth step is to understand the importance of your work.\\n\\nThe thirteenth step is to understand the importance of your work.\\n\\nThe th'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model_hf, tokenizer_hf, prompt=\"Every effort moves you\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "69074222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Encoding' object has no attribute 'eos_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvery effort moves you\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx_lm/generate.py:756\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, tokenizer, prompt, verbose, formatter, **kwargs)\u001b[39m\n\u001b[32m    753\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m10\u001b[39m)\n\u001b[32m    755\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx_lm/generate.py:660\u001b[39m, in \u001b[36mstream_generate\u001b[39m\u001b[34m(model, tokenizer, prompt, draft_model, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03mA generator producing text based on the given prompt from the model.\u001b[39;00m\n\u001b[32m    643\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    657\u001b[39m \u001b[33;03m        associated metadata. See :class:`GenerationResponse` for details.\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, TokenizerWrapper):\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m     tokenizer = \u001b[43mTokenizerWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, mx.array):\n\u001b[32m    663\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    664\u001b[39m         \u001b[38;5;66;03m# Try to infer if special tokens are needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llms-from-scratch-mlx/.venv/lib/python3.13/site-packages/mlx_lm/tokenizer_utils.py:269\u001b[39m, in \u001b[36mTokenizerWrapper.__init__\u001b[39m\u001b[34m(self, tokenizer, detokenizer_class, eos_token_ids)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m._tokenizer = tokenizer\n\u001b[32m    265\u001b[39m \u001b[38;5;28mself\u001b[39m._detokenizer = detokenizer_class(tokenizer)\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m._eos_token_ids = (\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mset\u001b[39m(eos_token_ids)\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eos_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m {\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m}\n\u001b[32m    270\u001b[39m )\n\u001b[32m    271\u001b[39m \u001b[38;5;28mself\u001b[39m._think_start = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m._think_end = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Encoding' object has no attribute 'eos_token_id'"
     ]
    }
   ],
   "source": [
    "generate(model, tokenizer, prompt=\"Every effort moves you\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-from-scratch-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
