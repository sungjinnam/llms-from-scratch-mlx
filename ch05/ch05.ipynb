{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4738cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c0622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bf8ad",
   "metadata": {},
   "source": [
    "# Evaluating text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ff7693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(50257, 768)\n",
       "  (drop_emb): Dropout(p=0.09999999999999998)\n",
       "  (trf_blocks): Sequential(\n",
       "    (layers.0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_key): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (W_value): Linear(input_dims=768, output_dims=768, bias=False)\n",
       "        (out_proj): Linear(input_dims=768, output_dims=768, bias=True)\n",
       "        (dropout): Dropout(p=0.09999999999999998)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (layers.0): Linear(input_dims=768, output_dims=3072, bias=True)\n",
       "          (layers.1): GELU()\n",
       "          (layers.2): Linear(input_dims=3072, output_dims=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.09999999999999998)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(input_dims=768, output_dims=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12, \n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1, \n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "\n",
    "mx.random.seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe17449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = mx.array(encoded)[None, :]\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze().tolist()\n",
    "    return tokenizer.decode(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4a1830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShot'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7a4c6",
   "metadata": {},
   "source": [
    "## Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32900a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mx.array([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                   [40, 1107, 588]])      #  \"I really like\"]\n",
    "targets = mx.array([[3626, 6100, 345],    # [\" effort moves you\"\n",
    "                    [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d29abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[2.4659e-05, 7.09139e-06, 5.94067e-05, ..., 8.50546e-06, 3.1108e-05, 3.1253e-05],\n",
       "         [2.19308e-05, 7.08788e-06, 4.74957e-05, ..., 9.22388e-06, 1.9442e-05, 1.9591e-05],\n",
       "         [1.83429e-05, 8.42821e-06, 4.95861e-05, ..., 9.82874e-06, 2.17547e-05, 1.95116e-05]],\n",
       "        [[2.73172e-05, 9.37263e-06, 1.38779e-05, ..., 2.61956e-05, 4.36025e-05, 7.52438e-06],\n",
       "         [2.4238e-05, 8.87468e-06, 1.3109e-05, ..., 3.36475e-05, 3.15629e-05, 7.75805e-06],\n",
       "         [2.17332e-05, 8.64109e-06, 1.32396e-05, ..., 3.29413e-05, 2.869e-05, 8.55843e-06]]], dtype=float32),\n",
       " (2, 3, 50257))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(mx.stop_gradient(inputs))\n",
    "probas = nn.softmax(logits, axis=-1)\n",
    "probas, probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1a6058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1158],\n",
       "        [40697],\n",
       "        [17149]],\n",
       "       [[30526],\n",
       "        [29643],\n",
       "        [46678]]], dtype=uint32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = mx.argmax(probas, axis=-1, keepdims=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac0c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: ves paed icons\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e525799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: array([1.26521e-05, 6.10094e-05, 9.74481e-06], dtype=float32)\n",
      "Text 2: array([8.53397e-06, 1.31845e-05, 1.62757e-05], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b911e1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11.2777, -9.70448, -11.5388, -11.6715, -11.2365, -11.0258], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to maximize the log probability of the targets\n",
    "log_probas = mx.log(mx.concat([target_probas_1, target_probas_2], axis=0))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb698e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-11.0758, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probas = mx.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d92363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(11.0758, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in training, we minimize the negative log likelihood\n",
    "neg_avg_log_probas = -avg_log_probas\n",
    "neg_avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9198c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 50257), (6,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.flatten(0, 1).shape, targets.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d63b4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(11.0758, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.losses.cross_entropy(logits.flatten(0, 1), targets.flatten(),\n",
    "                               reduction=\"mean\")\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02a1d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(64588.1, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity is exp of the cross-entropy loss\n",
    "perplexity = mx.exp(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390277b6",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5a62088",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ch02/the-verdict.txt\", \"r\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adedbea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no ',\n",
       " 'it for me! The Strouds stand alone, and happen once--but there\\'s no exterminating our kind of art.\"')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:99], text_data[-99:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b27811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20479, 5145)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_characters, total_tokens = len(text_data), len(tokenizer.encode(text_data))\n",
    "total_characters, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1aa9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "\n",
      "Validation loader:\n",
      "(2, 256) (2, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4608, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from previous_chapters import gpt_dataset_v1\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "mx.random.seed(123)\n",
    "train_loader = gpt_dataset_v1(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = gpt_dataset_v1(\n",
    "    val_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the trainig loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "    \n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")\n",
    "print(\"Train loader:\")\n",
    "\n",
    "train_tokens = 0\n",
    "for ex in iter(train_loader[0]):\n",
    "    print(ex['input_ids'].shape, ex['target_ids'].shape)\n",
    "    train_tokens += ex['input_ids'].size\n",
    "\n",
    "val_tokens = 0\n",
    "print(\"\\nValidation loader:\")\n",
    "for ex in iter(val_loader[0]):\n",
    "    print(ex['input_ids'].shape, ex['target_ids'].shape)\n",
    "    val_tokens += ex['input_ids'].size\n",
    "\n",
    "train_tokens, val_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f3cad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce_loss(logits, target):\n",
    "    loss = nn.losses.cross_entropy(logits.flatten(0, 1), target.flatten(), \n",
    "                                   reduction=\"mean\")\n",
    "    return loss\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, no_grad, device=None):\n",
    "    input_batch, target_batch = mx.array(input_batch), mx.array(target_batch, dtype=mx.int32)\n",
    "\n",
    "    if no_grad:\n",
    "        input_batch = mx.stop_gradient(input_batch)\n",
    "    logits = model(input_batch)\n",
    "    loss, grad = nn.value_and_grad(model, compute_ce_loss)(logits, target_batch)\n",
    "    return loss, grad\n",
    "\n",
    "def calc_loss_loader(data_loader, model, no_grad, device=None, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if data_loader[1] == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = data_loader[1]\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, data_loader[1])\n",
    "    for i, ex in enumerate(iter(data_loader[0])):\n",
    "        if i < num_batches:\n",
    "            loss, _ = calc_loss_batch(ex['input_ids'], ex['target_ids'], model, no_grad, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70255990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.531623946295844, 5.520528793334961)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(123)\n",
    "# TODO: shouldn't require to start the dataloader again\n",
    "train_loader = gpt_dataset_v1(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = gpt_dataset_v1(\n",
    "    val_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "train_loss = calc_loss_loader(train_loader, model, True)\n",
    "val_loss = calc_loss_loader(val_loader, model, True)\n",
    "train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad4453",
   "metadata": {},
   "source": [
    "# Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd156eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, \n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TODO: shouldn't require to start the dataloader again\n",
    "        train_loader = gpt_dataset_v1(\n",
    "            train_data,\n",
    "            tokenizer,\n",
    "            GPT_CONFIG_124M[\"context_length\"],\n",
    "            GPT_CONFIG_124M[\"context_length\"],\n",
    "            batch_size=2,\n",
    "            shuffle=True,\n",
    "        )        \n",
    "        model.train()\n",
    "        for ex in iter(train_loader[0]):\n",
    "            input_batch, target_batch = ex['input_ids'], ex['target_ids']\n",
    "            loss, grad = calc_loss_batch(input_batch, target_batch, model, False)\n",
    "            optimizer.update(model, grad)\n",
    "            # Force a graph evaluation\n",
    "            mx.eval(model.parameters(), optimizer.state)\n",
    "            train_loss = loss.item()\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            tokens_seen += input_batch.size\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                # train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1}, step {global_step:06d}: \"\n",
    "                      f\"train loss {train_loss:.3f}, val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    val_loader = gpt_dataset_v1(\n",
    "        val_data,\n",
    "        tokenizer,\n",
    "        GPT_CONFIG_124M[\"context_length\"],\n",
    "        GPT_CONFIG_124M[\"context_length\"],\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    # train_loss = calc_loss_loader(train_loader, model, True, device, eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, True, device, eval_iter)\n",
    "    model.train()\n",
    "    return val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=mx.stop_gradient(encoded),\n",
    "        max_new_tokens=50,\n",
    "        context_size=context_size,\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d5ff2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 000005: train loss 11.018, val loss 5.520\n",
      "Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShotaterasuShotaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE hrs hrsaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE\n",
      "Epoch 2, step 000010: train loss 11.047, val loss 5.520\n",
      "Epoch 2, step 000015: train loss 11.034, val loss 5.520\n",
      "Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShotaterasuShotaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE hrs hrsaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE\n",
      "Epoch 3, step 000020: train loss 11.038, val loss 5.520\n",
      "Epoch 3, step 000025: train loss 11.043, val loss 5.520\n",
      "Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShotaterasuShotaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE hrs hrsaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE\n",
      "Epoch 4, step 000030: train loss 11.038, val loss 5.520\n",
      "Epoch 4, step 000035: train loss 11.003, val loss 5.520\n",
      "Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShotaterasuShotaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE hrs hrsaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE\n",
      "Epoch 5, step 000040: train loss 11.096, val loss 5.520\n",
      "Epoch 5, step 000045: train loss 11.047, val loss 5.520\n",
      "Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShotaterasuShotaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE hrs hrsaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE\n",
      "Epoch 6, step 000050: train loss 11.061, val loss 5.520\n",
      "Every effort moves you rational ErraterasuOregonOregon Stevenson bur StevensonOregonShotaterasuShotaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE hrs hrsaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasuaterasu BRE\n",
      "Epoch 7, step 000055: train loss 11.080, val loss 5.520\n",
      "Epoch 7, step 000060: train loss 11.055, val loss 5.520\n",
      "Every effort moves you rational NichaterasuOregon peaks Stevenson Stevenson Stevenson StevensonShot StevensonShot Stevenson Stevenson Stevensonaterasuaterasuaterasuaterasuotingotingotingotingotingotingotingotingotingotingotingotingoting quotedotingoting quoted quotedoting quotedoting BREoting quotedotingRen BREoting hrs BRE BRE\n",
      "Epoch 8, step 000065: train loss 11.092, val loss 5.519\n",
      "Epoch 8, step 000070: train loss 11.052, val loss 5.519\n",
      "Every effort moves you rational NichaterasuOregon peaks Stevenson Stevenson Stevenson StevensonShot StevensonShot Stevenson Stevenson Stevensonaterasuaterasuaterasuaterasuotingotingotingotingotingotingotingotingotingotingotingotingoting quotedotingoting quoted quotedoting quotedoting BREoting quotedotingRen BREoting hrs BRE BRE\n",
      "Epoch 9, step 000075: train loss 11.110, val loss 5.519\n",
      "Epoch 9, step 000080: train loss 11.031, val loss 5.519\n",
      "Every effort moves you rational NichaterasuOregon peaks Stevenson Stevenson Stevenson StevensonShot StevensonShot Stevenson Stevenson Stevensonaterasuaterasuaterasuaterasuotingotingotingotingotingotingotingotingotingotingotingotingoting quotedotingoting quoted quotedoting quotedoting BREoting quotedotingRen BREoting hrs BRE BRE\n",
      "Epoch 10, step 000085: train loss 11.064, val loss 5.519\n",
      "Epoch 10, step 000090: train loss 11.056, val loss 5.519\n",
      "Every effort moves you rational NichaterasuOregon peaks Stevenson Stevenson Stevenson StevensonShot StevensonShot Stevenson Stevenson Stevensonaterasuaterasuaterasuaterasuotingotingotingotingotingotingotingotingotingotingotingotingoting quotedotingoting quoted quotedoting quotedoting BREoting quotedotingRen BREoting hrs BRE BRE\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "optimizer = optim.AdamW(learning_rate=4e-4, weight_decay=1e-1)\n",
    "\n",
    "# TODO: shouldn't require to start the dataloader again\n",
    "train_loader = gpt_dataset_v1(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = gpt_dataset_v1(\n",
    "    val_data,\n",
    "    tokenizer,\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    GPT_CONFIG_124M[\"context_length\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, None, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74195dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ed255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ffa02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-from-scratch-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
